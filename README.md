# ğŸ“ Student Dropout Prediction â€“ End-to-End Machine Learning Project

An end-to-end *Machine Learning classification project* that predicts whether a student will *Dropout, **Stay Enrolled, or **Graduate*, using real-world higher education data.  
This project covers the *complete ML lifecycle*, from raw data to deployment.

---

## ğŸ“Œ Project Overview

Student dropout is a critical issue in higher education, affecting students, institutions, and society.  
This project aims to *predict student academic outcomes early*, allowing institutions to take preventive actions.

### ğŸ¯ Problem Type
- *Multi-class Classification*
- Classes:
  - âŒ Dropout
  - ğŸ“ Graduate

---

## ğŸ§  Key Highlights

- âœ” Built *from scratch*
- âœ” Real-world educational dataset
- âœ” Strong feature engineering
- âœ” Multiple ML models compared
- âœ” Hyperparameter tuning with pipelines
- âœ” Deployment-ready architecture
- âœ” API + Frontend + Dockerized

---

## ğŸ“‚ Dataset Information

- *Source:* UCI Machine Learning Repository  
- *Initial Features:* 37  
- *Final Features Used:* 18  
- *Target Variable:* Target  
- *Missing Values:*  None  

ğŸ“Œ A *detailed dataset description* is available in:  
ğŸ‘‰ DATASET_DESCRIPTION.md

---

## ğŸ§¹ Data Cleaning & Feature Engineering

### ğŸ”¹ Raw Dataset
- Contained *37 features*
- Many features were:
  - Not always available
  - Redundant
  - Weakly correlated with the target

### ğŸ”¹ Feature Engineering Strategy
- Removed low-impact and rarely available features
- Merged related academic attributes
- Retained only *highly informative & practical features*
- Final dataset reduced to *18 strong features*

ğŸ“Œ Result:
> *Cleaner data + better generalization + realistic deployment*

---

## ğŸ” Exploratory Data Analysis (EDA)

Performed deep EDA to understand:
- Dropout patterns
- Academic performance trends
- Financial & socio-economic impact
- Feature importance signals

### ğŸ“Š Visual Analysis Included
- Target class distribution
- Correlation heatmaps
- Feature-wise comparisons
- Performance-based insights

---

## âš™ï¸ Machine Learning Pipeline

### ğŸ”¹ Models Trained
The following *4 classification models* were trained and evaluated:

| Model | Description |
|-----|------------|
| Logistic Regression | Baseline linear model |
| Random Forest | Tree-based ensemble |
| Support Vector Machine (SVM) | Margin-based classifier |
| XGBoost | Gradient boosting model |

---

## ğŸ§ª Hyperparameter Tuning

- Created *separate pipelines* for each model
- Used *hyperparameter tuning* to optimize performance
- Ensured fair and consistent comparison

ğŸ“Œ All models were trained on the *same processed dataset*

---

## ğŸ“ˆ Model Evaluation & Comparison

### ğŸ”¹ Metrics Used
- Accuracy
- Precision
- Recall
- F1-score

### ğŸ”¹ Visualization
- ğŸ“Š Accuracy comparison charts
- ğŸ“Š Precision comparison charts

---

## ğŸ“Š Model Training & Evaluation

I compared four different machine learning models and developed Hyperparameter Tuning Pipelines for each to ensure optimal performance::

| Model | Accuracy | Precision | Status |
| :--- | :---: | :---: | :--- |
| **Logistic Regression** | 85% | 84% | Baseline |
| **Random Forest** | 88% | 87% | Strong Competitor |
| **SVM** | 84% | 83% | Robust |
| **XGBoost ğŸ†** | **90%** | **89%** | **Final Selected Model** |

### ğŸ“ˆ Why XGBoost?
XGBoost delivered the best performance with an accuracy of 90%. It excels at capturing complex non-linear relationships within the data while effectively controlling for overfitting. Furthermore, its high precision is crucial for accurately identifying students at risk of dropping out, minimizing false alarms.

### âœ… *Final Selected Model*
| Model | Accuracy |
|-----|----------|
| *XGBoost* | *90%* |

ğŸ“Œ *Why XGBoost?*
- Highest accuracy
- Better generalization
- Strong handling of feature interactions
- Best fit for this problem

---

## ğŸ’¾ Model Saving

- Saved:
  - Trained models
  - Test datasets
- Ensured reproducibility
- Ready for deployment & inference

---

## ğŸš€ API Development (FastAPI)

- Built a *REST API* using *FastAPI*
- Handles:
  - Input validation
  - Model inference
  - JSON-based responses

ğŸ“Œ API allows external systems to use the model independently.

---

## ğŸ–¥ï¸ Frontend (Streamlit)

- Developed a *simple & clean frontend*
- Users can:
  - Enter student details
  - Get real-time predictions
- Designed for *demo & usability*

---

## ğŸ³ Dockerization

- Created a *Dockerfile*
- Entire application is:
  - Portable
  - Environment-independent
  - Easy to deploy

ğŸ“Œ Ensures consistency across systems.

---

## ğŸ› ï¸ Tech Stack
* **Backend:** FastAPI (Python)
* **Frontend:** Streamlit
* **Machine Learning:** XGBoost, Scikit-Learn, Pandas, NumPy
* **Visualization:** Matplotlib, Seaborn
* **Deployment:** Docker
* **Model Saving:** Joblib

---

## ğŸ§© Project Architecture

```text
ğŸ“‚ STUDENT-DROPOUT-RISK-PREDICTION/
â”‚
â”œâ”€â”€ ğŸ“‚ api/
â”‚   â””â”€â”€ ğŸ“„ main.py
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“„ raw-data.csv
â”‚   â”œâ”€â”€ ğŸ“„ cleaned-data.csv
â”‚   â””â”€â”€ ğŸ“„ featured-data.csv
â”‚
â”œâ”€â”€ ğŸ“‚ frontend/
â”‚   â””â”€â”€ ğŸ“„ app.py
â”‚
â”œâ”€â”€ ğŸ“‚ models & test data/
â”‚   â”œâ”€â”€ ğŸ“‚ logistic-regression/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logistic-regression-model.pkl
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logistic-regression-X_test.pkl
â”‚   â”‚   â””â”€â”€ ğŸ“„ logistic-regression-y_test.pkl
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ random-forest/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ random-forest-model.pkl
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ random-forest-X_test.pkl
â”‚   â”‚   â””â”€â”€ ğŸ“„ random-forest-y_test.pkl
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ svm/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ svm-model.pkl
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ svm-X_test.pkl
â”‚   â”‚   â””â”€â”€ ğŸ“„ svm-y_test.pkl
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ xgboost/
â”‚       â”œâ”€â”€ ğŸ“„ xgboost-model.pkl
â”‚       â”œâ”€â”€ ğŸ“„ xgboost-X_test.pkl
â”‚       â””â”€â”€ ğŸ“„ xgboost-y_test.pkl
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â”œâ”€â”€ ğŸ“‚ Automated-EDA Results/
â”‚   â”œâ”€â”€ ğŸ“‚ AutoViz_Plots/
â”‚   â”œâ”€â”€ ğŸ“„ Pandas_Profiling.html
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ EDA/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ automated-EDA.ipynb
â”‚   â”‚   â””â”€â”€ ğŸ“„ manual-EDA.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ Model Training/
â”‚       â”œâ”€â”€ ğŸ“„ logistic-regression.ipynb
â”‚       â”œâ”€â”€ ğŸ“„ random-forest.ipynb
â”‚       â”œâ”€â”€ ğŸ“„ SVM.ipynb
â”‚       â”œâ”€â”€ ğŸ“„ XGBoost.ipynb
â”‚       â”œâ”€â”€ ğŸ“„ data-cleaning.ipynb
â”‚       â”œâ”€â”€ ğŸ“„ feature-engineering.ipynb
â”‚       â””â”€â”€ ğŸ“„ models-evaluation-comparison.ipynb
â”‚
â”œâ”€â”€ ğŸ“„ dataset-description.md
â””â”€â”€ ğŸ“„ README.md
